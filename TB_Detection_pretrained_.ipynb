{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TB_Detection_pretrained_.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttfMdvC_P-o4",
        "colab_type": "code",
        "outputId": "cf1e4f2c-6f79-4934-d7f7-c89a42e4ec5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9b5vCCrOUnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import os, sys, random\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_KCsu1zUj83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extract labels from filenane- 0:Normal, 1:Abnormal \n",
        "def get_label(filename):\n",
        "  return int(filename.split(\".png\")[0][-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95HrFE4CXDtW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Segregating Normal and Abnormal cases\n",
        "\n",
        "dataset_root = \"/content/drive/My Drive/ChinaSet_Dataset/CXR_png/\"\n",
        "filelist = os.listdir(dataset_root)\n",
        "filelist.sort()\n",
        "\n",
        "pos_ex = []\n",
        "neg_ex = []\n",
        "\n",
        "for file in filelist:\n",
        "  label = get_label(file)\n",
        "  if(label == 0):\n",
        "    pos_ex.append(file)\n",
        "  else:\n",
        "    neg_ex.append(file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CnLoXkTr3XU",
        "colab_type": "code",
        "outputId": "4e390b3b-6fb7-49ea-e3e5-591d482a18b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(\"Total No. of examples in the dataset {}\".format(len(filelist)))\n",
        "print(\"No. of Normal cases {}\".format(len(pos_ex)))\n",
        "print(\"No. of Abnormal cases {}\".format(len(neg_ex)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total No. of examples in the dataset 662\n",
            "No. of Normal cases 326\n",
            "No. of Abnormal cases 336\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DP6V1s38tIQH",
        "colab_type": "code",
        "outputId": "0e426fb3-4218-4f1e-abcc-b8b0c2e5248c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Creating Test list\n",
        "# we will use 90% data to train and remaining 10% to test\n",
        "No_Test_ex = int(len(filelist)*0.1) # 66 samples for test\n",
        "\n",
        "# we will use 33 Normal and Abnormal samples respectively for testing\n",
        "# we could have randomly chosen 66 samples from the dataset but that would not \n",
        "# gaurantee sampling equal no. of positve and negative cases.\n",
        "test_pos_list = random.sample(pos_ex,int(No_Test_ex/2))\n",
        "test_neg_list = random.sample(neg_ex,int(No_Test_ex/2))\n",
        "\n",
        "test_list = test_pos_list + test_neg_list\n",
        "test_list.sort()\n",
        "print(\"Total No. of test examples {}\".format(len(test_list)))\n",
        "\n",
        "\n",
        "# Creating Train list\n",
        "train_list = []\n",
        "\n",
        "for file in filelist:\n",
        "  if(file not in test_list):\n",
        "    train_list.append(file)\n",
        "train_list.sort()    \n",
        "print(\"Total No. of train examples {}\".format(len(train_list)))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total No. of test examples 66\n",
            "Total No. of train examples 596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iqv65BzJvMw1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating trainlist.txt and testlist.txt\n",
        "\n",
        "filetrain = open(\"/content/drive/My Drive/train_list.txt\",\"w+\")\n",
        "\n",
        "for file in train_list:\n",
        "  filetrain.write(file+\"\\n\")\n",
        "filetrain.close()\n",
        "\n",
        "filetest = open(\"/content/drive/My Drive/test_list.txt\",\"w+\")\n",
        "\n",
        "for file in test_list:\n",
        "  filetest.write(file+\"\\n\")\n",
        "filetest.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TlxgwPQmpj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_size = 128\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bt_5eVLKOhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def default_list_reader(filelist):\n",
        "  imglist = []\n",
        "  with open(filelist,'r') as file:\n",
        "    for line in file.readlines():\n",
        "      imgname,_ = line.split(\"\\n\")\n",
        "      imglist.append(imgname)\n",
        "  imglist.sort()\n",
        "  return imglist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNNKTDeMGU5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class My_Dataset(Dataset):\n",
        "  def __init__(self, root_dir, filelist = \" \", transform = transforms.ToTensor(), list_reader = default_list_reader):\n",
        "    self.root_dir = root_dir\n",
        "    self.img_names = list_reader(filelist)\n",
        "    self.labels = [get_label(img) for img in self.img_names]\n",
        "    self.transform = transform\n",
        "    \n",
        "  def __getitem__(self,index):\n",
        "    img = Image.open(self.root_dir+self.img_names[index])\n",
        "    img = self.transform(img)\n",
        "    return img,self.labels[index]\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.img_names)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO_y3h3Xm11v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_root = \"/content/drive/My Drive/ChinaSet_Dataset/CXR_png/\"\n",
        "train_list_path = \"/content/drive/My Drive/train_list.txt\"\n",
        "test_list_path = \"/content/drive/My Drive/test_list.txt\"\n",
        "\n",
        "train_set = My_Dataset(root_dir=dataset_root, filelist = train_list_path, transform = train_transform)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=8, shuffle=True)\n",
        "\n",
        "test_set = My_Dataset(root_dir=dataset_root, filelist = test_list_path, transform = test_transform)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=8, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvZJM09WTLYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(net, self).__init__()\n",
        "        self.C1 = torch.nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, padding=1, stride=1)\n",
        "        self.model_ft = torchvision.models.resnet18(pretrained=True)\n",
        "        self.model_ft.avgpool = torch.nn.AvgPool2d(kernel_size=4, padding=0, stride=2)\n",
        "        self.model_ft.fc = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512,256),\n",
        "            torch.nn.Linear(256,2)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        y = self.C1(x)\n",
        "        for _, layer in enumerate(list(self.model_ft.children())[:9]):\n",
        "            y = layer(y)\n",
        "        y = y.squeeze(-1).squeeze(-1)\n",
        "        y = list(self.model_ft.children())[-1](y)\n",
        "        return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zioeBAvQ5oAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epochs = 50\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "Model = net().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(Model.parameters(), lr=6e-4, weight_decay=1e-2)\n",
        "exp_lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6gsWAjn5_0W",
        "colab_type": "code",
        "outputId": "ebeb1339-dea8-4acb-e2fd-bdadc998ce4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "Model"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "net(\n",
              "  (C1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (model_ft): ResNet(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AvgPool2d(kernel_size=4, stride=2, padding=0)\n",
              "    (fc): Sequential(\n",
              "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (1): Linear(in_features=256, out_features=2, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iidd6ypw6GQV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss_track = []\n",
        "test_loss_track = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKnI6KbB7xJa",
        "colab_type": "code",
        "outputId": "69196648-b401-45a4-c48b-7ffd063a6e91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for eph in range(n_epochs):\n",
        "    print('epoch : {} ...'.format(eph))\n",
        "    n_correct = 0\n",
        "    avg_loss = 0\n",
        "    n_samples = 0\n",
        "    Model.train()\n",
        "    exp_lr_scheduler.step()\n",
        "    for idx, xy in enumerate(train_loader):\n",
        "        x, y = xy\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs = Model(x)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        n_correct += torch.sum(preds.data == y.data)\n",
        "        avg_loss += loss.item()\n",
        "        n_samples += x.size(0)\n",
        "    avg_loss = avg_loss/n_samples\n",
        "    train_loss_track.append(avg_loss)\n",
        "    print('train avg loss : ', avg_loss)\n",
        "    print('num of correct samples : {}/{}'.format(n_correct, n_samples))\n",
        "    \n",
        "    n_correct = 0\n",
        "    avg_loss = 0\n",
        "    n_samples = 0\n",
        "    gt_labels = []\n",
        "    pred_labels = []\n",
        "    Model.eval()\n",
        "    for idx, xy in enumerate(test_loader):\n",
        "        x, y = xy\n",
        "        x, y = x.cuda(), y.cuda()\n",
        "        outputs = Model(x)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, y)\n",
        "        \n",
        "        n_correct += torch.sum(preds.data == y.data)\n",
        "        gt_labels += list(y.data.cpu().numpy())\n",
        "        pred_labels += list(preds.data.cpu().numpy())\n",
        "        avg_loss += loss.item()\n",
        "        n_samples += x.size(0)\n",
        "    avg_loss = avg_loss/n_samples\n",
        "    test_loss_track.append(avg_loss)\n",
        "    print('test avg loss : ', avg_loss)\n",
        "    print('num of correct samples : {}/{}'.format(n_correct, n_samples))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch : 0 ...\n",
            "train avg loss :  0.07989015097686109\n",
            "num of correct samples : 421/596\n",
            "test avg loss :  0.10642124836643536\n",
            "num of correct samples : 50/66\n",
            "epoch : 1 ...\n",
            "train avg loss :  0.06885337929597637\n",
            "num of correct samples : 451/596\n",
            "test avg loss :  0.09299044107848947\n",
            "num of correct samples : 50/66\n",
            "epoch : 2 ...\n",
            "train avg loss :  0.06269629479654683\n",
            "num of correct samples : 468/596\n",
            "test avg loss :  0.07975184488477129\n",
            "num of correct samples : 44/66\n",
            "epoch : 3 ...\n",
            "train avg loss :  0.058476873317220866\n",
            "num of correct samples : 470/596\n",
            "test avg loss :  0.09746974277676958\n",
            "num of correct samples : 47/66\n",
            "epoch : 4 ...\n",
            "train avg loss :  0.05461362561823538\n",
            "num of correct samples : 485/596\n",
            "test avg loss :  0.14536728190653253\n",
            "num of correct samples : 34/66\n",
            "epoch : 5 ...\n",
            "train avg loss :  0.05708697855772588\n",
            "num of correct samples : 482/596\n",
            "test avg loss :  0.08955457174416745\n",
            "num of correct samples : 42/66\n",
            "epoch : 6 ...\n",
            "train avg loss :  0.05747786016772257\n",
            "num of correct samples : 476/596\n",
            "test avg loss :  0.08072475840648015\n",
            "num of correct samples : 48/66\n",
            "epoch : 7 ...\n",
            "train avg loss :  0.054659703521420494\n",
            "num of correct samples : 488/596\n",
            "test avg loss :  0.09055674030925288\n",
            "num of correct samples : 51/66\n",
            "epoch : 8 ...\n",
            "train avg loss :  0.060570243571028616\n",
            "num of correct samples : 483/596\n",
            "test avg loss :  0.07710618254813281\n",
            "num of correct samples : 51/66\n",
            "epoch : 9 ...\n",
            "train avg loss :  0.05716838880173312\n",
            "num of correct samples : 489/596\n",
            "test avg loss :  0.08553642169995741\n",
            "num of correct samples : 49/66\n",
            "epoch : 10 ...\n",
            "train avg loss :  0.060121199553644894\n",
            "num of correct samples : 486/596\n",
            "test avg loss :  0.09099272299896587\n",
            "num of correct samples : 44/66\n",
            "epoch : 11 ...\n",
            "train avg loss :  0.05169284960017508\n",
            "num of correct samples : 493/596\n",
            "test avg loss :  3.4623662775213067\n",
            "num of correct samples : 33/66\n",
            "epoch : 12 ...\n",
            "train avg loss :  0.059110136977678175\n",
            "num of correct samples : 495/596\n",
            "test avg loss :  0.08059912784533067\n",
            "num of correct samples : 53/66\n",
            "epoch : 13 ...\n",
            "train avg loss :  0.05825778216923643\n",
            "num of correct samples : 489/596\n",
            "test avg loss :  0.1328796035412586\n",
            "num of correct samples : 33/66\n",
            "epoch : 14 ...\n",
            "train avg loss :  0.05846694661866898\n",
            "num of correct samples : 489/596\n",
            "test avg loss :  0.08374270571000648\n",
            "num of correct samples : 49/66\n",
            "epoch : 15 ...\n",
            "train avg loss :  0.05567090283304253\n",
            "num of correct samples : 504/596\n",
            "test avg loss :  0.11280698577562968\n",
            "num of correct samples : 33/66\n",
            "epoch : 16 ...\n",
            "train avg loss :  0.05615276503282905\n",
            "num of correct samples : 483/596\n",
            "test avg loss :  0.07456620342352173\n",
            "num of correct samples : 55/66\n",
            "epoch : 17 ...\n",
            "train avg loss :  0.05850856738702563\n",
            "num of correct samples : 488/596\n",
            "test avg loss :  0.08460332927378741\n",
            "num of correct samples : 50/66\n",
            "epoch : 18 ...\n",
            "train avg loss :  0.05270387694839663\n",
            "num of correct samples : 496/596\n",
            "test avg loss :  0.10693349824710326\n",
            "num of correct samples : 45/66\n",
            "epoch : 19 ...\n",
            "train avg loss :  0.05441417317562455\n",
            "num of correct samples : 496/596\n",
            "test avg loss :  0.08665497239791986\n",
            "num of correct samples : 49/66\n",
            "epoch : 20 ...\n",
            "train avg loss :  0.053493617240014495\n",
            "num of correct samples : 495/596\n",
            "test avg loss :  0.06613606079058214\n",
            "num of correct samples : 55/66\n",
            "epoch : 21 ...\n",
            "train avg loss :  0.05176710612421868\n",
            "num of correct samples : 499/596\n",
            "test avg loss :  0.07394239599957611\n",
            "num of correct samples : 53/66\n",
            "epoch : 22 ...\n",
            "train avg loss :  0.051075912687002414\n",
            "num of correct samples : 505/596\n",
            "test avg loss :  0.06255454412012389\n",
            "num of correct samples : 56/66\n",
            "epoch : 23 ...\n",
            "train avg loss :  0.04917936904618404\n",
            "num of correct samples : 495/596\n",
            "test avg loss :  0.0892070136738546\n",
            "num of correct samples : 49/66\n",
            "epoch : 24 ...\n",
            "train avg loss :  0.04752827839513353\n",
            "num of correct samples : 502/596\n",
            "test avg loss :  0.06784101259527785\n",
            "num of correct samples : 54/66\n",
            "epoch : 25 ...\n",
            "train avg loss :  0.04899898718967534\n",
            "num of correct samples : 492/596\n",
            "test avg loss :  0.07556898801615744\n",
            "num of correct samples : 48/66\n",
            "epoch : 26 ...\n",
            "train avg loss :  0.044110897578449056\n",
            "num of correct samples : 504/596\n",
            "test avg loss :  0.06370097850308273\n",
            "num of correct samples : 54/66\n",
            "epoch : 27 ...\n",
            "train avg loss :  0.04842171755513089\n",
            "num of correct samples : 501/596\n",
            "test avg loss :  0.06380569426850839\n",
            "num of correct samples : 56/66\n",
            "epoch : 28 ...\n",
            "train avg loss :  0.045384222841602845\n",
            "num of correct samples : 505/596\n",
            "test avg loss :  0.13803597483219524\n",
            "num of correct samples : 42/66\n",
            "epoch : 29 ...\n",
            "train avg loss :  0.04504848136597832\n",
            "num of correct samples : 511/596\n",
            "test avg loss :  0.07136947356841782\n",
            "num of correct samples : 45/66\n",
            "epoch : 30 ...\n",
            "train avg loss :  0.04297907021671733\n",
            "num of correct samples : 520/596\n",
            "test avg loss :  0.06912473261807904\n",
            "num of correct samples : 57/66\n",
            "epoch : 31 ...\n",
            "train avg loss :  0.036995978288402495\n",
            "num of correct samples : 527/596\n",
            "test avg loss :  0.062461260480411125\n",
            "num of correct samples : 54/66\n",
            "epoch : 32 ...\n",
            "train avg loss :  0.030544694363070814\n",
            "num of correct samples : 538/596\n",
            "test avg loss :  0.05404250210884846\n",
            "num of correct samples : 54/66\n",
            "epoch : 33 ...\n",
            "train avg loss :  0.03302806008282124\n",
            "num of correct samples : 538/596\n",
            "test avg loss :  0.050430468819809685\n",
            "num of correct samples : 55/66\n",
            "epoch : 34 ...\n",
            "train avg loss :  0.02477435848906936\n",
            "num of correct samples : 552/596\n",
            "test avg loss :  0.05580674224730694\n",
            "num of correct samples : 52/66\n",
            "epoch : 35 ...\n",
            "train avg loss :  0.02739126180242372\n",
            "num of correct samples : 552/596\n",
            "test avg loss :  0.07607985999096524\n",
            "num of correct samples : 54/66\n",
            "epoch : 36 ...\n",
            "train avg loss :  0.029274724386382425\n",
            "num of correct samples : 544/596\n",
            "test avg loss :  0.050807771357623016\n",
            "num of correct samples : 56/66\n",
            "epoch : 37 ...\n",
            "train avg loss :  0.020554203165057523\n",
            "num of correct samples : 565/596\n",
            "test avg loss :  0.050179972341566376\n",
            "num of correct samples : 54/66\n",
            "epoch : 38 ...\n",
            "train avg loss :  0.012871752986871956\n",
            "num of correct samples : 575/596\n",
            "test avg loss :  0.0654016708334287\n",
            "num of correct samples : 52/66\n",
            "epoch : 39 ...\n",
            "train avg loss :  0.013400821865305005\n",
            "num of correct samples : 581/596\n",
            "test avg loss :  0.03479287017023924\n",
            "num of correct samples : 59/66\n",
            "epoch : 40 ...\n",
            "train avg loss :  0.009293696380551629\n",
            "num of correct samples : 582/596\n",
            "test avg loss :  0.05222038055459658\n",
            "num of correct samples : 57/66\n",
            "epoch : 41 ...\n",
            "train avg loss :  0.010077439573587187\n",
            "num of correct samples : 582/596\n",
            "test avg loss :  0.039176891021656265\n",
            "num of correct samples : 60/66\n",
            "epoch : 42 ...\n",
            "train avg loss :  0.007231595557208029\n",
            "num of correct samples : 589/596\n",
            "test avg loss :  0.043172336889035774\n",
            "num of correct samples : 57/66\n",
            "epoch : 43 ...\n",
            "train avg loss :  0.009067348635836735\n",
            "num of correct samples : 583/596\n",
            "test avg loss :  0.04464097366188512\n",
            "num of correct samples : 58/66\n",
            "epoch : 44 ...\n",
            "train avg loss :  0.004171133303902294\n",
            "num of correct samples : 592/596\n",
            "test avg loss :  0.04263839012745655\n",
            "num of correct samples : 59/66\n",
            "epoch : 45 ...\n",
            "train avg loss :  0.005517334208492465\n",
            "num of correct samples : 589/596\n",
            "test avg loss :  0.0416849000435887\n",
            "num of correct samples : 59/66\n",
            "epoch : 46 ...\n",
            "train avg loss :  0.00548714862498861\n",
            "num of correct samples : 589/596\n",
            "test avg loss :  0.04065325551412322\n",
            "num of correct samples : 59/66\n",
            "epoch : 47 ...\n",
            "train avg loss :  0.004239472348067984\n",
            "num of correct samples : 593/596\n",
            "test avg loss :  0.03904458315986575\n",
            "num of correct samples : 58/66\n",
            "epoch : 48 ...\n",
            "train avg loss :  0.0067768924863346475\n",
            "num of correct samples : 591/596\n",
            "test avg loss :  0.04366513702905539\n",
            "num of correct samples : 58/66\n",
            "epoch : 49 ...\n",
            "train avg loss :  0.004454987048602744\n",
            "num of correct samples : 593/596\n",
            "test avg loss :  0.04006645241470048\n",
            "num of correct samples : 58/66\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuRUgFfV76MN",
        "colab_type": "code",
        "outputId": "6414a9fd-4010-44b4-f8b2-45214a693ca3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plt.plot(train_loss_track, 'b')\n",
        "plt.plot(test_loss_track, 'r')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('avg loss')\n",
        "plt.show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFMzMbNj/B/OS9U9eWvpzAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p90uK1CG79_k",
        "colab_type": "code",
        "outputId": "66f57c85-d720-4b20-fdaf-2c8f35218a51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "target_names = ['No TB', 'TB']\n",
        "print(classification_report(gt_labels, pred_labels, target_names=target_names))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       No TB       0.93      0.82      0.87        33\n",
            "          TB       0.84      0.94      0.89        33\n",
            "\n",
            "    accuracy                           0.88        66\n",
            "   macro avg       0.88      0.88      0.88        66\n",
            "weighted avg       0.88      0.88      0.88        66\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YtXZEms9Kc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_save_name = 'model_res_pre.pt' \n",
        "path = F\"/content/drive/My Drive/{model_save_name}\" \n",
        "torch.save(Model.state_dict(), path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPgPjhssz2zJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}